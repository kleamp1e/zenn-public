---
title: "けしからん画像分類器を作ってみる (2) データ収集"
emoji: "👙"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["machinelearning", "deeplearning", "computervision", "python"]
published: false
---

# 目次

* [けしからん画像分類器を作ってみる (1) 序章](https://zenn.dev/kleamp1e/articles/202102-pornography-classifier-1)
* けしからん画像分類器を作ってみる (2) データ収集

# 画像分類するまでの流れ

[前回](https://zenn.dev/kleamp1e/articles/202102-pornography-classifier-1)、テーマを「けしからん画像分類器」に決めました。
まずは、画像分類するまでの流れを確認してみましょう。（順序は前後することもあります）

1. ツールを選定する。
2. ライブラリを選定する。
3. 指標を設定する。
4. データを収集する。
5. ラベル付けする。
6. 学習する。
7. 評価する。
8. 活用する。

今回は(1)〜(4)を見ていきたいと思います。

# (1) ツールを選定する

第3次AIブームが訪れて早10数年。世の中には機械学習関連のツールが溢れかえっています。
今回取り組む「画像分類」タスクのためのツールもいろいろあり、一般的な処理を行うのであれば1行のコーディングすら不要です。
比較的新しいものでは、Microsoft社が2020年10月に公開した「[Lobe](https://lobe.ai/)」（ローブ）があります。参考に、公式の紹介動画を貼っておきます。使ったことはまだありませんが、手軽そうですね。

@[youtube](Mdcw3Sb98DA)

ただ、今回の目的は「画像処理と機械学習について学ぶこと」なので、ゴリゴリとコーディングすることにします。つまり使うツールは「プログラミング言語、機械学習ライブラリ、エディタなどなど」です。

# (2) ライブラリを選定する

ツールと同様、画像処理、機械学習に関連するライブラリも沢山存在します。
高レベルな（より抽象度が高い）ライブラリほどコーディング量が少ない代わりに自由度が低く、逆に低レベルな（より抽象度が低い）ライブラリほどコーディング量が多い代わりに自由度が高い傾向にあります。
例えば「物体検出」タスクを行う場合は、Facebook社の「[detectron2](https://github.com/facebookresearch/detectron2)」などを使うことで、短いコードでタスクを実現することができます。

ただ、今回はより深く機械学習について学び、自分の手に馴染んだ道具にするのが目的なので、あまり高レベルなライブラリは使わないことにします。
そもそも、画像分類は比較的単純なタスクなので、高レベルなライブラリを使う必要がない、というのもありますが。

比較的低レベルなライブラリとしては、以下の二大巨頭があります。

* Google社の[TensorFlow](https://www.tensorflow.org/)
* Facebook社の[PyTorch](https://pytorch.org/)

今回は上記のどちらかを使うことにします。もう少し先、実装に入る時に決めますが、もしかしたらTensorFlowに含まれるKerasを使うかも知れません。こらそこ「結局、高レベルライブラリを使うのかよ」とか言わない。

画像処理に関しては、深く考えずに[OpenCV](https://opencv.org/)を使うことにします。実績も情報量も多いので。

# (3) 指標を設定する

機械学習に限らず、問題を解く時には「どうやって問題が上手く解けたことを説明するか？（表現するか？）」が重要となります。
今回は画像分類タスク、しかも二項分類（Binary classification、「二値分類」とも、[Wikipedia](https://ja.wikipedia.org/wiki/%E4%BA%8C%E9%A0%85%E5%88%86%E9%A1%9E)）なので話は単純で「正解率」（Accuracy）を用います。

データに偏りがある場合、正解率では適切に表現できないこともあるので注意が必要です。例えば10日に1回しか雨が降らない地域で「晴れか？雨か？」の天気予報を行う場合、常に「晴れ」と答えるだけで正解率が90%になってしまう、などの例があります。

今回は「世の中に存在する画像の内、50%はけしからん画像」という仮定を置き…というのはさすがに乱暴過ぎるので、テストデータ（学習に用いず、評価にだけ用いるデータ）に含まれるデータを50%ずつにすることで対処します。
詳しくは「不均衡データ」（Imbalanced data）とかで調べると良いと思います。

# (4) データを収集する

ツール、ライブラリ、指標の選定、設定を経て、ついに楽しい楽しい「データ収集」の時間です。
「機械学習はデータの量がものを言う」と言われるくらいにデータは重要です。（実際には「適切にラベル付けされたデータ」であることが多いですが）

そんなわけで沢山データを集めます。選別は後ですれば良いので、とにかく沢山集めましょう。いや、本当はデータ収集についての計画も重要なんですけどね。

ふつうのプログラマであれば、データを集めるのには大した労力はいりません。サクッとスクレイピングツール（クローラ）を書いて実行するだけです。
頻繁に1つのサイトからデータを収集すると怒られたり訴えられたりするかも知れないので、ゆっくり時間がお掛けて実行しましょう。対象ホスト毎に間隔を開けるようにしておくと良いでしょう。たまに集まったデータを覗いたりして楽しみながら待ちましょう。

♪3分クッキングの音楽♪

さて、データが集まりました。今回は約48万枚の静止画、1,700本の短い動画を集めました。
インターネットにはけしからん画像、動画ばっかりなので作業が捗ります。今回は動画は使いませんが、将来のためについでに集めておきました。

ここでふと、困ったことに気付きます。「けしからん画像」を集めるのは容易なのですが、「けしからんくない画像」をどうやって集めれば良いのだろう？…と。
けしからんサイトの対極に位置しそうなので「NHKのサイトに載っている画像を使おう」とも思いましたが、受信料を請求されても嫌なので、「けしからん画像サイト」に含まれる「けしからんくない画像」を使うことにしました。
要するにラベル付けで頑張る、ってことですね。まあ、それも勉強と言うことで。

今回はここまで。僕らの戦いは始まったばかりだ！
